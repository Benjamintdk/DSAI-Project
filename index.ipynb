{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from DSAI_proj.extract import *\n",
    "from DSAI_proj.clean import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE1115 DSAI Project \n",
    "\n",
    "> Data Science Project to build a movie tagline predictor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project motivation and utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ever found yourself staring at a certain movie tagline and wondering what on earth the directors were thinking about when they came up with it? Yes, we too. On the other hand, we've also often been on the other end of the spectrum, struggling to come up with creative titles for our projects or succinct one-liners to encapsulate our business pitches. Enter our movie tagline predictor, a model we have built which we hope will provide some enlightenment to the many lost souls described above. We believe that this model can be transferrable to other salient applications, such as the web-link summarization we often see accompanying our everyday Google searches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package install:\n",
    "Run the following command to install the package from PyPI or conda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install DSAI_proj`\n",
    "\n",
    "`conda install DSAI_proj`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editable Install:\n",
    "1) Clone the repository locally and cd into it. \n",
    "\n",
    "2) Create a virtual/conda/pipenv environment first.\n",
    "\n",
    "3) Depending on which type of environment you are using, run one of the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install -e`\n",
    "\n",
    "`conda develop .`\n",
    "\n",
    "`pipenv install -e`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function in this package perform the following functions for data extraction via the TMDB API:\n",
    "- Multi-threaded download of movie information via the extract_dataset_threaded function.\n",
    "- Saving the data to a raw_data.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our raw dataset to work with, we begin by looking at:\n",
    "- The different types of data present.\n",
    "- How they might be relevant to our task.\n",
    "- Their respective distributions and whether there might be class imbalances.\n",
    "- Missing data and other considerations to be had later during cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the EDA step, we have generated insights from the data in terms of their distribution and relevance our overall task. The functions in this package we perform the following functions to clean and feature engineer the raw data:\n",
    "- Drop columns which are irrelevant.\n",
    "- For columns/features with json objects, unpack the json objects into new columns.\n",
    "- For categorical features, ensure that they are categorized accordingly.\n",
    "- For datetime features, ensure that they are separated out into year, month and day before either being rescaled or binned.\n",
    "- Separate out examples with and without taglines.\n",
    "- Save the dataset into separate train, valid, test and tagless csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next create the Dataset class to encapsulate all the different types of data which we will be working with. We also add in augmentations and transformations including:\n",
    "- Tokenization of text based on the text based encoder which we will be using (DistilBert).\n",
    "- RandomResizeCrop to resize all images to a standard size and crop them randomly.\n",
    "- Normalization and Standardization of all images on the imagenet dataset statistics.\n",
    "- Converting all features to Pytorch tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](model_diagram.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, we model the problem by creating the architectures and modules required to map the various inputs to the predicted tagline. In our case, we will be using an encoder-decoder based structure, with the encoder consisting of separate networks for image, text and meta/tabular data respectively, while the decoder consists of a few transformer decoder layers and a linear layer to output the predicted sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leverage the famous FastAI library to help us with training. The library provides us with numerous benefits, including advanced training techniques using learning rate schedulers and discriminative learning rates, along with seamless integration of features such as mixed precision training in the case where our model might be too big for GPU memory.\n",
    "\n",
    "Here, we train our model and validate it on the validation dataset to give ourselves a good idea of how our model might perform in the real world. Then we save the model to use it for further testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results interpretation and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run evaluation on our test set, as well as predict some examples from the tagless csv to see how our model might perform in real life. \n",
    "\n",
    "We interpret the results and provide recommendations/suggestions for how we could further improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAI-Project",
   "language": "python",
   "name": "dsai-project"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
