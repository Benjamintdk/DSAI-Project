# AUTOGENERATED! DO NOT EDIT! File to edit: 04_model.ipynb (unless otherwise specified).

__all__ = ['cnn_encoder', 'text_encoder', 'meta_encoder', 'decoder']

# Internal Cell

from transformers import DistilBertModel, DistilBertTokenizer
from .dataset import *
from torch import nn
from fastai.data.core import DataLoaders
from functools import partial
from PIL import Image
import torchvision.models as models

# Internal Cell

def freeze_all_but_layer(m, layer):
    if not isinstance(m, layer):
        if hasattr(m, 'weight') and m.weight is not None:
            m.weight.requires_grad_(False)
        if hasattr(m, 'bias') and m.bias is not None:
            m.bias.requires_grad_(False)

# Cell

def cnn_encoder(pretrained: bool, in_channels: int, out_channels: int):
    model = models.resnet18(pretrained=pretrained)
    last_layers = [nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=False), nn.AdaptiveAvgPool2d(1)]
    model = nn.Sequential(*list(model.children())[:-2], *last_layers)
    img_freeze_fn = partial(freeze_all_but_layer, layer=nn.BatchNorm2d)
    model.apply(img_freeze_fn)
    return model

# Cell

def text_encoder(model_type: str):
    model = DistilBertModel.from_pretrained(model_type)
    text_freeze_fn = partial(freeze_all_but_layer, layer=nn.LayerNorm)
    model.apply(text_freeze_fn)
    return model

# Cell

def meta_encoder(in_channels: int, out_channels: int):
    model = nn.Linear(in_features=in_channels, out_features=out_channels)
    return model

# Cell

def decoder(hidden_dim: int, vocab_size: int):
    return nn.Linear(in_features=hidden_dim, out_features=vocab_size)