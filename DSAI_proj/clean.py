# AUTOGENERATED! DO NOT EDIT! File to edit: 02_clean.ipynb (unless otherwise specified).

__all__ = ['extract_images_threaded', 'split_datetime', 'remove_sw', 'drop_col', 'create_splits']

# Internal Cell
from pandas import DataFrame
from sklearn.utils import shuffle
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.preprocessing import MultiLabelBinarizer
from .extract import *
from .eda import *
import string
import re
import os
import pandas as pd
import json
import concurrent
import requests
import pandas as pd
import numpy as np

# Internal Cell

def req_image(url: str, save_path: str, id_num: int):
    req_url = f"https://image.tmdb.org/t/p/original{url}"
    response = requests.get(req_url)
    if response.status_code == 200:
        fname = os.path.join(save_path, f"{id_num}.jpg")
        with open(fname, "wb") as f:
            f.write(response.content)
            return None
    return id_num

# Cell

def extract_images_threaded(df: DataFrame,
                            cur_path: str,
                            img_type: list,
                            max_threads: int) -> tuple:
    max_threads = max_threads if max_threads < len(df) else len(df)
    problem_ids = []
    for itype in img_type:
        save_path = os.path.join(cur_path, f"{itype}_img")
        os.makedirs(save_path, exist_ok=True)

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:
            pids = [id_num for cnt, url in enumerate(df[f"{itype}_path"]) if (id_num := executor.submit(req_image, url, save_path, df.iloc[cnt]['id']).result()) is not None]
        problem_ids.extend(pids)
        print(f"{itype} images written successfully!")
    problem_ids = set(problem_ids)
    df = df[~df['id'].isin(problem_ids)]
    return df, problem_ids

# Cell

def split_datetime(df: DataFrame,
                   date_col: str) -> DataFrame:
    df[date_col] = pd.to_datetime(df[date_col], format='%Y-%m-%d')
    df[f"{date_col}_year"] = df[date_col].dt.year
    df[f"{date_col}_month"] = df[date_col].dt.month
    df[f"{date_col}_day"] = df[date_col].dt.day
    df.drop(date_col, inplace=True, axis=1)

    day_bins = [0, 10, 20, 31]
    day_labels = [1, 2, 3]
    year_bins = [1900, 1940, 1960, 1980, 2000, 2020]
    year_labels = [1, 2, 3, 4, 5]
    df[f"{date_col}_day"]  = pd.cut(df[f"{date_col}_day"], bins=day_bins, labels=day_labels)
    df[f"{date_col}_year"] = pd.cut(df[f"{date_col}_year"], bins=year_bins, labels=year_labels)
    return df

# Cell

# remove stopwords & punctuation
def remove_sw(df: DataFrame, sw: set, var: list, keep_missing: bool, save_path: str = "."):
    if keep_missing:
        unlabelled_df = df[(df["tagline"] == '') | (df["tagline"] == ' ')]
        unlabelled_df.to_csv(os.path.join(save_path, "tagless.csv"))
        print(f"Tagless set size: {len(unlabelled_df)}")
        print("Tagless dataset created!")
    df = df[(df["tagline"] != '') & (df["tagline"] != ' ')]
    df = df[df['tagline'].notna()]
    for v in var:
        df[v] = df[v].apply(lambda x: str.lower(x))
        df[v] = df[v].apply(lambda x: " ".join([word for word in word_tokenize(x) if word not in sw]))
    return df

# Internal Cell

def drop_missing(df: DataFrame, cols: list) -> DataFrame:
    df.dropna(subset=cols, inplace=True)
    return df

# Cell

def drop_col(data: DataFrame,
             irrelevant_cols: list,
             relevant_cols: list,
             sw: set,
             var: list) -> DataFrame:
    df = data.drop(irrelevant_cols, axis = 1)
    df = drop_missing(df=df, cols=relevant_cols)
    df = split_datetime(df=df, date_col="release_date")
    df = remove_sw(df=df, sw=sw, var=var, keep_missing=True)
    df = df.drop(df.columns[0], axis=1)
    return df

# Cell

def create_splits(df: DataFrame,
                  label: str,
                  splits: list,
                  seed: int,
                  save_path: str = "."):

    assert len(splits) == 2, "Train, validation and test splits must be provided, please provide 2 of them as fractions."
    df_size = len(df)
    labelled_df = shuffle(df, random_state=seed)
    labelled_df.reset_index(inplace=True, drop=True)
    valid_start, test_start = int(df_size*splits[0]), int(df_size*splits[0] + df_size*splits[1])
    train_df = labelled_df.iloc[:valid_start]
    valid_df = labelled_df.iloc[valid_start:test_start]
    test_df = labelled_df[test_start:]
    print(f"Train set size: {len(train_df)}\nValid set size: {len(valid_df)}\nTest set size: {len(test_df)}")
    train_df.to_csv(os.path.join(save_path, "train.csv"), index=False)
    valid_df.to_csv(os.path.join(save_path, "valid.csv"), index=False)
    test_df.to_csv(os.path.join(save_path, "test.csv"), index=False)
    print("Train, Validation and Test datasets created!")