{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "> The TMDB dataset is only accessible through an API. Hence, we will write a few functions to extract the dataset, perform some simple preprocessing before saving to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract the dataset by accessing the TMDB API. Since not all requests will be successful, a larger than desired dataset size value should be used to return an eventual dataset size of roughly the same magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import concurrent\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def extract_dataset(api_key: str, n: int):\n",
    "    req_url = f\"https://api.themoviedb.org/3/movie/{n}?api_key={api_key}&language=en-US\"\n",
    "    response = requests.get(req_url)\n",
    "    if response.status_code == 200:\n",
    "        array = response.json()\n",
    "        return array\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def extract_dataset_threaded(api_key: str, \n",
    "                             max_ds_size: int, \n",
    "                             max_threads: int) -> DataFrame:\n",
    "    \n",
    "    max_threads = max_threads if max_threads < max_ds_size else ds_size\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "        movies = [ex for n in range(max_ds_size) if (ex := executor.submit(extract_dataset, api_key, n).result()) is not None]\n",
    "    return pd.DataFrame.from_records(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the data extraction functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "api_key = \"785475a69770b7dc1af964feff948dd7\"\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 6)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples downloaded is: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key = \"your_api_key_here\"\n",
    "max_ds_size = 1000\n",
    "max_threads = 1000\n",
    "movies = extract_dataset_threaded(api_key=api_key,\n",
    "                                  max_ds_size=max_ds_size,\n",
    "                                  max_threads=max_threads)\n",
    "print(f\"Total number of examples downloaded is: {len(movies)}\")\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first 5 rows of the scraped dataset, we easily see that some example taglines (under the tagline column) are missing. We'll need to separate these examples into a separate csv file as they do not have labels. Additionally, we can create our train, validation and test datasets concurrently and save them into separate csv files. This helps reproducibility later on. We also print the relative proportions of each dataset to see if we will need to redo the extraction process above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def create_splits(df: DataFrame,\n",
    "                  label: str,\n",
    "                  splits: list,\n",
    "                  seed: int,\n",
    "                  keep_missing: bool,\n",
    "                  save_path: str = \".\"):\n",
    "\n",
    "    assert len(splits) == 2, \"Train, validation and test splits must be provided, please provide 2 of them as fractions.\"\n",
    "    if keep_missing:\n",
    "        unlabelled_df = df[df[label] == '']\n",
    "        unlabelled_df.to_csv(os.path.join(save_path, \"tagless.csv\"))\n",
    "        print(f\"Tagless set size: {len(unlabelled_df)}\")\n",
    "        print(\"Tagless dataset created!\")\n",
    "    labelled_df = df[df.tagline != '']\n",
    "    df_size = len(labelled_df)\n",
    "    labelled_df = shuffle(labelled_df, random_state=seed)\n",
    "    labelled_df.reset_index(drop=True, inplace=True)\n",
    "    valid_start, test_start = int(df_size*splits[0]), int(df_size*splits[0] + df_size*splits[1])\n",
    "    train_df = labelled_df.iloc[:valid_start]\n",
    "    valid_df = labelled_df.iloc[valid_start:test_start]\n",
    "    test_df = labelled_df[test_start:]\n",
    "    print(f\"Train set size: {len(train_df)}\\nValid set size: {len(valid_df)}\\nTest set size: {len(test_df)}\")\n",
    "    train_df.to_csv(os.path.join(save_path, \"train.csv\"))\n",
    "    valid_df.to_csv(os.path.join(save_path, \"valid.csv\"))\n",
    "    test_df.to_csv(os.path.join(save_path, \"test.csv\"))\n",
    "    print(\"Train, Validation and Test datasets created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [0.7, 0.15]\n",
    "label = \"tagline\"\n",
    "seed = 42\n",
    "create_splits(df=movies,\n",
    "              label=label,\n",
    "              splits=splits,\n",
    "              seed=seed,\n",
    "              keep_missing=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAI-Project",
   "language": "python",
   "name": "dsai-project"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
