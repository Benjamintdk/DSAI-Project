{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ca8eb",
   "metadata": {},
   "source": [
    "# Model\n",
    "> Our project will require the use of 4 separate types of models: an image model, a text model, a tabular model, and a decoder network. The relationship between the 4 can be seen in the figure below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ecd15e",
   "metadata": {},
   "source": [
    "![](model_diagram.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730428ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from DSAI_proj.dataset import *\n",
    "from torch import nn\n",
    "from fastai.data.core import DataLoaders\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def freeze_all_but_layer(m, layer):\n",
    "    if not isinstance(m, layer):\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            m.weight.requires_grad_(False)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            m.bias.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61659c",
   "metadata": {},
   "source": [
    "We first design a cnn_encoder module using a pretrained resnet 18 architecture. We will keep the weights frozen as we do not want them to be updated too much in the training process. We also unfreeze the batchnorm layers, as these have been shown to learn the distributions better when unfrozen during fine-tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def cnn_encoder(pretrained: bool, in_channels: int, out_channels: int):\n",
    "    model = models.resnet18(pretrained=pretrained)\n",
    "    last_layers = [nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=False), nn.AdaptiveAvgPool2d(1)]\n",
    "    model = nn.Sequential(*list(model.children())[:-2], *last_layers)\n",
    "    img_freeze_fn = partial(freeze_all_but_layer, layer=nn.BatchNorm2d)\n",
    "    model.apply(img_freeze_fn)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfdaec4",
   "metadata": {},
   "source": [
    "Next, we design a text_encoder module, which will consist of the encoder layers of DistilBert. Similar to the cnn_encoder, we freeze all the layers except the normalization layers, which in this case is LayerNorm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def text_encoder(model_type: str):\n",
    "    model = DistilBertModel.from_pretrained(model_type)\n",
    "    text_freeze_fn = partial(freeze_all_but_layer, layer=nn.LayerNorm) \n",
    "    model.apply(text_freeze_fn)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f3798",
   "metadata": {},
   "source": [
    "We will also need a module for our tabular meta data, and hence use a simple linear layer which will map the input meta data to necessary output shape required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74744a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def meta_encoder(in_channels: int, out_channels: int):\n",
    "    model = nn.Linear(in_features=in_channels, out_features=out_channels)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d4f7d",
   "metadata": {},
   "source": [
    "The last piece of the puzzle is a decoder network that will decode the outputs of the above 3 encoder modules and produce the predicted score where the last dimension represents the vocabulary size of the model. In other words, these are the raw logits distributed across all possible words, and a softmax will be applied to determine the most likely word. For our case, we will use a simple linear layer to act as the decoder layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def decoder(hidden_dim: int, vocab_size: int):\n",
    "    return nn.Linear(in_features=hidden_dim, out_features=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e55dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaglinePredictorModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int, meta_features: int):\n",
    "        super(TaglinePredictorModel, self).__init__()\n",
    "        self.cnn_encoder = cnn_encoder(pretrained=True, in_channels=512, out_channels=768)\n",
    "        self.text_encoder = text_encoder(model_type='distilbert-base-uncased')\n",
    "        self.meta_encoder = meta_encoder(in_channels=meta_features, out_channels=768)\n",
    "        self.decoder = decoder(hidden_dim=768, vocab_size=vocab_size)\n",
    "        \n",
    "    def forward(self, x: dict):\n",
    "        poster_feature = self.cnn_encoder(x['poster_img']).squeeze(-1).permute(0, 2, 1)\n",
    "        backdrop_feature = self.cnn_encoder(x['backdrop_img']).squeeze(-1).permute(0, 2, 1)\n",
    "        text_feature = self.text_encoder((x['text_inputs'])).last_hidden_state\n",
    "        meta_feature = self.meta_encoder(x['meta']).unsqueeze(1)\n",
    "        \n",
    "        poster_feat = self.decoder(poster_feature)\n",
    "        backdrop_feat = self.decoder(backdrop_feature)\n",
    "        text_feat = self.decoder(text_feature)\n",
    "        meta_feat = self.decoder(meta_feature)\n",
    "        return (poster_feat, backdrop_feat, text_feat, meta_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079a9cf",
   "metadata": {},
   "source": [
    "The values for this Tagline model are mostly hard-coded as we are limited by architectural choices. As we will be using DistilBert for the text encoder, our hidden dimensions are limited to being 768, with a vocabulary size of 30522. Our choice of architecture for the image encoder is the resnet 18, which has a final output channel dimension of 512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac2276",
   "metadata": {},
   "source": [
    "Now that we are finally finished creating the model class, let's test it out on an example from the dataset created in the previous section. We'll additionally be testing out the FastAI dataloader as the model will be receiving the data directly from the dataloader during training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e228f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset creation\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_length = 80\n",
    "height = width = 128\n",
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9bc60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset created!\n",
      "Shape of poster image is torch.Size([1, 1, 30522])\n",
      "Shape of backdrop image is torch.Size([1, 1, 30522])\n",
      "Shape of text feature is torch.Size([1, 80, 30522])\n",
      "Shape of meta features is torch.Size([1, 1, 30522])\n"
     ]
    }
   ],
   "source": [
    "tfms = Compose([Tokenize(tokenizer=tokenizer, max_length=max_length),\n",
    "                         RandomResizeCrop(width=width, height=height, method=Image.BILINEAR),\n",
    "                         ToTensor(),\n",
    "                         NormalizeStandardize(mean=mean, std=std)])\n",
    "\n",
    "poster_img_dir = \"poster_img/\"\n",
    "backdrop_img_dir = \"backdrop_img/\"\n",
    "train_ds = MovieDataset(poster_img_dir=poster_img_dir,\n",
    "                        backdrop_img_dir=backdrop_img_dir,\n",
    "                        ds_type=\"train\",\n",
    "                        transforms=tfms)\n",
    "dls = DataLoaders.from_dsets(train_ds, batch_size=1)\n",
    "sample = dls.one_batch()\n",
    "model = TaglinePredictorModel(vocab_size=30522, meta_features=sample['meta'].shape[-1])\n",
    "res1, res2, res3, res4 = model(sample)\n",
    "print(f\"Shape of poster image is {res1.shape}\")\n",
    "print(f\"Shape of backdrop image is {res2.shape}\")\n",
    "print(f\"Shape of text feature is {res3.shape}\")\n",
    "print(f\"Shape of meta features is {res4.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5060f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAI-Project",
   "language": "python",
   "name": "dsai-project"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
