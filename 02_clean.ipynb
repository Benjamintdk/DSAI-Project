{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-aberdeen",
   "metadata": {},
   "source": [
    "# Data Cleaning and Feature Engineering\n",
    "> Cleaning and feature engineering based on the insights gained from the previous step on EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from pandas import DataFrame\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from DSAI_proj.extract import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import concurrent\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "api_key = \"785475a69770b7dc1af964feff948dd7\"\n",
    "max_ds_size = 20\n",
    "max_threads = 8\n",
    "movies = extract_dataset_threaded(api_key=api_key,\n",
    "                                  max_ds_size=max_ds_size,\n",
    "                                  max_threads=max_threads,\n",
    "                                  save_path='.',\n",
    "                                  fname='raw_data.csv')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-artwork",
   "metadata": {},
   "source": [
    "The first step will be to process the categorical variables of the dataset. In this case, we have genres as a categorical variable. We will use the MultiLabelBinarizer from scikit-learn to one-hot-encode the movie genres. This will create additional columns in our DataFrame, each corresponding to a separate genre type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def clean_genre(df: DataFrame)-> DataFrame:\n",
    "    \n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    df['genres'] = [[x['name'] for x in list_dict] for list_dict in df['genres']]\n",
    "    df1 = df.join(\n",
    "                pd.DataFrame.sparse.from_spmatrix(\n",
    "                mlb.fit_transform(df.pop('genres')),\n",
    "                index=df.index,\n",
    "                columns=mlb.classes_))\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>backdrop_path</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>homepage</th>\n",
       "      <th>...</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Science Fiction</th>\n",
       "      <th>Thriller</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>/hQ4pYsIbP22TMXOUdSfC2mjWrO0.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>/l94l89eMmFKh7na2a1u5q67VgNx.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>/u0zMKKpEdDWpOKmFW2sLbKKICJH.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>4000000</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>/5aXp2s4l6g5PcMMesIj63mx8hmJ.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>21000000</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>42000</td>\n",
       "      <td>http://www.lifeinloops.com</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                     backdrop_path belongs_to_collection    budget  \\\n",
       "0  False  /hQ4pYsIbP22TMXOUdSfC2mjWrO0.jpg                  None         0   \n",
       "1  False  /l94l89eMmFKh7na2a1u5q67VgNx.jpg                  None         0   \n",
       "2  False  /u0zMKKpEdDWpOKmFW2sLbKKICJH.jpg                  None   4000000   \n",
       "3  False  /5aXp2s4l6g5PcMMesIj63mx8hmJ.jpg                  None  21000000   \n",
       "4  False                              None                  None     42000   \n",
       "\n",
       "                     homepage  ...  Horror Mystery Romance Science Fiction  \\\n",
       "0                              ...       0       0       0               0   \n",
       "1                              ...       0       0       0               0   \n",
       "2                              ...       0       0       0               0   \n",
       "3                              ...       0       0       0               0   \n",
       "4  http://www.lifeinloops.com  ...       0       0       0               0   \n",
       "\n",
       "  Thriller  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        1  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clean_genre(df=movies)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-bermuda",
   "metadata": {},
   "source": [
    "Next, we will need to extract the images into a separate directory. This will allow for easie access during the training stage. The following codes help us to extract the backdrop and poster images from their urls to separate directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def extract_backdrop_img(df: DataFrame, save_path: str):\n",
    "    save_path = os.path.join(save_path, \"backdrop_img\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    for cnt,x in enumerate(df['backdrop_path']):\n",
    "        \n",
    "        if x is not None:\n",
    "            response = requests.get(\"https://image.tmdb.org/t/p/original\"+str(x))\n",
    "            with open(os.path.join(save_path, str(df['id'][cnt])+\".jpg\"), \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "    print(\"All backdrop images written successfully!\")\n",
    "    return\n",
    "\n",
    "def extract_poster_img(df: DataFrame, save_path: str):\n",
    "    save_path = os.path.join(save_path, \"poster_img\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    for cnt,x in enumerate(df['poster_path']):\n",
    "        \n",
    "        if x is not None:\n",
    "            response = requests.get(\"https://image.tmdb.org/t/p/original\"+str(x))\n",
    "            with open(os.path.join(save_path, str(df['id'][cnt])+\".jpg\"), \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "    print(\"All poster images written successfully!\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-forwarding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All backdrop images written successfully!\n",
      "All poster images written successfully!\n"
     ]
    }
   ],
   "source": [
    "extract_backdrop_img(df=df, save_path=\".\")\n",
    "extract_poster_img(df=df, save_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-server",
   "metadata": {},
   "source": [
    "Lastly, from the previous notebook on EDA, we have already identified the relevant and irrelevant features required for our tagline prediction task. We will now drop the columns or features that are irrelevant. We can also include the image url paths to be dropped as we have already extracted the necessary images into a separate folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def drop_col(data: DataFrame,\n",
    "             irrelevant_cols: list) -> DataFrame:\n",
    "    df = data.drop(irrelevant_cols,axis = 1)\n",
    "    df['release_date'] = pd.to_datetime(df['release_date'], format='%Y-%m-%d')\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-title",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>overview</th>\n",
       "      <th>release_date</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Science Fiction</th>\n",
       "      <th>Thriller</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Taisto Kasurinen is a Finnish coal miner whose...</td>\n",
       "      <td>1988-10-21</td>\n",
       "      <td></td>\n",
       "      <td>Ariel</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>An episode in the life of Nikander, a garbage ...</td>\n",
       "      <td>1986-10-17</td>\n",
       "      <td></td>\n",
       "      <td>Shadows in Paradise</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>It's Ted the Bellhop's first night on the job....</td>\n",
       "      <td>1995-12-09</td>\n",
       "      <td>Twelve outrageous guests. Four scandalous requ...</td>\n",
       "      <td>Four Rooms</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>While racing to a boxing match, Frank, Mike, J...</td>\n",
       "      <td>1993-10-15</td>\n",
       "      <td>Don't move. Don't whisper. Don't even breathe.</td>\n",
       "      <td>Judgment Night</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Timo Novotny labels his new project an experim...</td>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>A Megacities remix.</td>\n",
       "      <td>Life in Loops (A Megacities RMX)</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           overview release_date  \\\n",
       "0   2  Taisto Kasurinen is a Finnish coal miner whose...   1988-10-21   \n",
       "1   3  An episode in the life of Nikander, a garbage ...   1986-10-17   \n",
       "2   5  It's Ted the Bellhop's first night on the job....   1995-12-09   \n",
       "3   6  While racing to a boxing match, Frank, Mike, J...   1993-10-15   \n",
       "4   8  Timo Novotny labels his new project an experim...   2006-01-01   \n",
       "\n",
       "                                             tagline  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2  Twelve outrageous guests. Four scandalous requ...   \n",
       "3     Don't move. Don't whisper. Don't even breathe.   \n",
       "4                                A Megacities remix.   \n",
       "\n",
       "                              title  ...  Horror  Mystery  Romance  \\\n",
       "0                             Ariel  ...       0        0        0   \n",
       "1               Shadows in Paradise  ...       0        0        0   \n",
       "2                        Four Rooms  ...       0        0        0   \n",
       "3                    Judgment Night  ...       0        0        0   \n",
       "4  Life in Loops (A Megacities RMX)  ...       0        0        0   \n",
       "\n",
       "   Science Fiction  Thriller  \n",
       "0                0         0  \n",
       "1                0         0  \n",
       "2                0         0  \n",
       "3                0         1  \n",
       "4                0         0  \n",
       "\n",
       "[5 rows x 19 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irrelevant_columns = ['belongs_to_collection','homepage','imdb_id','production_companies','popularity','original_language','original_title','revenue','runtime','spoken_languages','status','video','vote_average','vote_count','production_countries','budget', 'poster_path', 'backdrop_path']\n",
    "df = drop_col(data=df, irrelevant_cols=irrelevant_columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-violation",
   "metadata": {},
   "source": [
    "The text will not require as much preprocessing as we will be using transformer based models to deal with the text data. We will look into that in greater detail later. Now that we are done with the basic preprocessing and feature engineering, we can finally create our train, validation and test splits in separate csv files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-indonesian",
   "metadata": {},
   "source": [
    "Looking at the first 5 rows of the cleaned dataset, we easily see that some example taglines (under the tagline column) are missing. We'll need to separate these examples into a separate csv file as they do not have labels. Additionally, we can create our train, validation and test datasets concurrently and save them into separate csv files. This helps reproducibility later on. We also print the relative proportions of each dataset to see if we will need to redo the extraction process above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def create_splits(df: DataFrame,\n",
    "                  label: str,\n",
    "                  splits: list,\n",
    "                  seed: int,\n",
    "                  keep_missing: bool,\n",
    "                  save_path: str = \".\"):\n",
    "\n",
    "    assert len(splits) == 2, \"Train, validation and test splits must be provided, please provide 2 of them as fractions.\"\n",
    "    if keep_missing:\n",
    "        unlabelled_df = df[df[label] == '']\n",
    "        unlabelled_df.to_csv(os.path.join(save_path, \"tagless.csv\"))\n",
    "        print(f\"Tagless set size: {len(unlabelled_df)}\")\n",
    "        print(\"Tagless dataset created!\")\n",
    "    labelled_df = df[df[label] != '']\n",
    "    df_size = len(labelled_df)\n",
    "    labelled_df = shuffle(labelled_df, random_state=seed)\n",
    "    valid_start, test_start = int(df_size*splits[0]), int(df_size*splits[0] + df_size*splits[1])\n",
    "    train_df = labelled_df.iloc[:valid_start]\n",
    "    valid_df = labelled_df.iloc[valid_start:test_start]\n",
    "    test_df = labelled_df[test_start:]\n",
    "    print(f\"Train set size: {len(train_df)}\\nValid set size: {len(valid_df)}\\nTest set size: {len(test_df)}\")\n",
    "    train_df.to_csv(os.path.join(save_path, \"train.csv\"))\n",
    "    valid_df.to_csv(os.path.join(save_path, \"valid.csv\"))\n",
    "    test_df.to_csv(os.path.join(save_path, \"test.csv\"))\n",
    "    print(\"Train, Validation and Test datasets created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-kingdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagless set size: 3\n",
      "Tagless dataset created!\n",
      "Train set size: 8\n",
      "Valid set size: 2\n",
      "Test set size: 2\n",
      "Train, Validation and Test datasets created!\n"
     ]
    }
   ],
   "source": [
    "splits = [0.7, 0.15]\n",
    "label = \"tagline\"\n",
    "seed = 42\n",
    "create_splits(df=df,\n",
    "              label=label,\n",
    "              splits=splits,\n",
    "              seed=seed,\n",
    "              keep_missing=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAI-Project",
   "language": "python",
   "name": "dsai-project"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
