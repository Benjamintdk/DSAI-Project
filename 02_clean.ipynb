{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a09324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-aberdeen",
   "metadata": {},
   "source": [
    "# Data Cleaning and Feature Engineering\n",
    "> Cleaning and feature engineering based on the insights gained from the previous step on EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e441aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from pandas import DataFrame\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from DSAI_proj.extract import *\n",
    "from DSAI_proj.eda import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import concurrent\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea105654",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('raw_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-artwork",
   "metadata": {},
   "source": [
    "The first step will be to process the categorical variables of the dataset. In this case, we have genres as a categorical variable. We will use the MultiLabelBinarizer from scikit-learn to one-hot-encode the movie genres. This will create additional columns in our DataFrame, each corresponding to a separate genre type. We will reuse the clean_genre function used in the EDA section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-information",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>adult</th>\n",
       "      <th>backdrop_path</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>...</th>\n",
       "      <th>TV Movie</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/hQ4pYsIbP22TMXOUdSfC2mjWrO0.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>/l94l89eMmFKh7na2a1u5q67VgNx.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>/u0zMKKpEdDWpOKmFW2sLbKKICJH.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>/5aXp2s4l6g5PcMMesIj63mx8hmJ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  adult                     backdrop_path belongs_to_collection  \\\n",
       "0           0  False  /hQ4pYsIbP22TMXOUdSfC2mjWrO0.jpg                   NaN   \n",
       "1           1  False  /l94l89eMmFKh7na2a1u5q67VgNx.jpg                   NaN   \n",
       "2           2  False  /u0zMKKpEdDWpOKmFW2sLbKKICJH.jpg                   NaN   \n",
       "3           3  False  /5aXp2s4l6g5PcMMesIj63mx8hmJ.jpg                   NaN   \n",
       "4           4  False                               NaN                   NaN   \n",
       "\n",
       "   ...  TV Movie Thriller  War Western  \n",
       "0  ...         0        0    0       0  \n",
       "1  ...         0        0    0       0  \n",
       "2  ...         0        0    0       0  \n",
       "3  ...         0        1    0       0  \n",
       "4  ...         0        0    0       0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clean_genre(df=movies)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-bermuda",
   "metadata": {},
   "source": [
    "Next, we will need to extract the images into a separate directory. This will allow for easier access during the training stage. The following codes help us to extract the backdrop and poster images from their urls to separate directories. We can multithread this function as well as it involves multiple IO operations. Additionally, as some examples will not have images, we will drop these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def req_image(url: str, save_path: str, id_num: int):\n",
    "    req_url = f\"https://image.tmdb.org/t/p/original{url}\"\n",
    "    response = requests.get(req_url)\n",
    "    if response.status_code == 200:\n",
    "        fname = os.path.join(save_path, f\"{id_num}.jpg\")\n",
    "        with open(fname, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "            return None\n",
    "    return id_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def extract_images_threaded(df: DataFrame,\n",
    "                            cur_path: str,\n",
    "                            img_type: list,\n",
    "                            max_threads: int) -> tuple:\n",
    "    max_threads = max_threads if max_threads < len(df) else len(df)\n",
    "    problem_ids = []\n",
    "    for itype in img_type:\n",
    "        save_path = os.path.join(cur_path, f\"{itype}_img\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "            pids = [id_num for cnt, url in enumerate(df[f\"{itype}_path\"]) if (id_num := executor.submit(req_image, url, save_path, df.iloc[cnt]['id']).result()) is not None]\n",
    "        problem_ids.extend(pids)\n",
    "        print(f\"{itype} images written successfully!\")\n",
    "    problem_ids = set(problem_ids)\n",
    "    df = df[~df['id'].isin(problem_ids)]\n",
    "    return df, problem_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-forwarding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped due to missing images: 1118\n"
     ]
    }
   ],
   "source": [
    "df, problem_ids = extract_images_threaded(df=df, cur_path=\".\", img_type=[\"poster\", \"backdrop\"], max_threads=1000)\n",
    "print(f\"Number of rows dropped due to missing images: {len(problem_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-server",
   "metadata": {},
   "source": [
    "Lastly, from the previous notebook on EDA, we have already identified the relevant and irrelevant features required for our tagline prediction task. We will now drop the columns or features that are irrelevant. We can also include the image url paths to be dropped as we have already extracted the necessary images into a separate folder. \n",
    "\n",
    "Additionally, we also feature engineer based on the release dates to split that column into year, month and day separately. This generally allows models to process such meta information better. We also bin the year and day as per the EDA; we will leave the months as it is as the values are generally small and in the range of the other variables.\n",
    "\n",
    "Speaking of dropped rows, let's also drop the rows which are missing information in the important columns identified during EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def drop_missing(df: DataFrame, cols: list) -> DataFrame:\n",
    "    df.dropna(subset=cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "def split_datetime(df: DataFrame,\n",
    "                   date_col: str) -> DataFrame:\n",
    "    df[date_col] = pd.to_datetime(df[date_col], format='%Y-%m-%d')\n",
    "    df[f\"{date_col}_year\"] = df[date_col].dt.year\n",
    "    df[f\"{date_col}_month\"] = df[date_col].dt.month\n",
    "    df[f\"{date_col}_day\"] = df[date_col].dt.day\n",
    "    df.drop(date_col, inplace=True, axis=1)\n",
    "    \n",
    "    day_bins = [0, 10, 20, 31]\n",
    "    day_labels = [1, 2, 3]\n",
    "    year_bins = [1900, 1940, 1960, 1980, 2000, 2020]\n",
    "    year_labels = [1, 2, 3, 4, 5]\n",
    "    df[f\"{date_col}_day\"]  = pd.cut(df[f\"{date_col}_day\"], bins=day_bins, labels=day_labels)\n",
    "    df[f\"{date_col}_year\"] = pd.cut(df[f\"{date_col}_year\"], bins=year_bins, labels=year_labels)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def drop_col(data: DataFrame,\n",
    "             irrelevant_cols: list,\n",
    "             relevant_cols: list) -> DataFrame:\n",
    "    df = data.drop(irrelevant_cols, axis = 1)\n",
    "    df = drop_missing(df=df, cols=relevant_cols)\n",
    "    df = split_datetime(df=df, date_col=\"release_date\")\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-title",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>id</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "      <th>...</th>\n",
       "      <th>Western</th>\n",
       "      <th>release_date_year</th>\n",
       "      <th>release_date_month</th>\n",
       "      <th>release_date_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>It's Ted the Bellhop's first night on the job....</td>\n",
       "      <td>Twelve outrageous guests. Four scandalous requ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>While racing to a boxing match, Frank, Mike, J...</td>\n",
       "      <td>Don't move. Don't whisper. Don't even breathe.</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>Timo Novotny labels his new project an experim...</td>\n",
       "      <td>A Megacities remix.</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>Princess Leia is captured and held hostage by ...</td>\n",
       "      <td>A long time ago in a galaxy far, far away...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>Nemo, an adventurous young clownfish, is unexp...</td>\n",
       "      <td>There are 3.7 trillion fish in the ocean. They...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult  id                                           overview  \\\n",
       "2  False   5  It's Ted the Bellhop's first night on the job....   \n",
       "3  False   6  While racing to a boxing match, Frank, Mike, J...   \n",
       "4  False   8  Timo Novotny labels his new project an experim...   \n",
       "6  False  11  Princess Leia is captured and held hostage by ...   \n",
       "7  False  12  Nemo, an adventurous young clownfish, is unexp...   \n",
       "\n",
       "                                             tagline  ... Western  \\\n",
       "2  Twelve outrageous guests. Four scandalous requ...  ...       0   \n",
       "3     Don't move. Don't whisper. Don't even breathe.  ...       0   \n",
       "4                                A Megacities remix.  ...       0   \n",
       "6       A long time ago in a galaxy far, far away...  ...       0   \n",
       "7  There are 3.7 trillion fish in the ocean. They...  ...       0   \n",
       "\n",
       "   release_date_year  release_date_month  release_date_day  \n",
       "2                  4                  12                 1  \n",
       "3                  4                  10                 2  \n",
       "4                  5                   1                 1  \n",
       "6                  3                   5                 3  \n",
       "7                  5                   5                 3  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irrelevant_columns = ['adult', 'belongs_to_collection','homepage','imdb_id','production_companies','popularity','original_language','original_title','revenue','runtime','spoken_languages','status','video','vote_average','vote_count','production_countries','budget', 'poster_path', 'backdrop_path']\n",
    "relevant_columns = list(set(df.columns) - set(irrelevant_columns))\n",
    "df = drop_col(data=df, irrelevant_cols=irrelevant_columns, relevant_cols=relevant_columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-violation",
   "metadata": {},
   "source": [
    "The text will not require as much preprocessing as we will be using transformer based models to deal with the text data. We will look into that in greater detail later. Now that we are done with the basic preprocessing and feature engineering, we can finally create our train, validation and test splits in separate csv files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-indonesian",
   "metadata": {},
   "source": [
    "Looking at the first 5 rows of the cleaned dataset, we easily see that some example taglines (under the tagline column) are missing. We'll need to separate these examples into a separate csv file as they do not have labels. Additionally, we can create our train, validation and test datasets concurrently and save them into separate csv files. This helps reproducibility later on. We also print the relative proportions of each dataset to see if we will need to redo the extraction process above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def create_splits(df: DataFrame,\n",
    "                  label: str,\n",
    "                  splits: list,\n",
    "                  seed: int,\n",
    "                  keep_missing: bool,\n",
    "                  save_path: str = \".\"):\n",
    "\n",
    "    assert len(splits) == 2, \"Train, validation and test splits must be provided, please provide 2 of them as fractions.\"\n",
    "    if keep_missing:\n",
    "        unlabelled_df = df[df[label] == '']\n",
    "        unlabelled_df.to_csv(os.path.join(save_path, \"tagless.csv\"))\n",
    "        print(f\"Tagless set size: {len(unlabelled_df)}\")\n",
    "        print(\"Tagless dataset created!\")\n",
    "    labelled_df = df[df[label] != '']\n",
    "    df_size = len(labelled_df)\n",
    "    labelled_df = shuffle(labelled_df, random_state=seed)\n",
    "    labelled_df.reset_index(inplace=True, drop=True)\n",
    "    valid_start, test_start = int(df_size*splits[0]), int(df_size*splits[0] + df_size*splits[1])\n",
    "    train_df = labelled_df.iloc[:valid_start]\n",
    "    valid_df = labelled_df.iloc[valid_start:test_start]\n",
    "    test_df = labelled_df[test_start:]\n",
    "    print(f\"Train set size: {len(train_df)}\\nValid set size: {len(valid_df)}\\nTest set size: {len(test_df)}\")\n",
    "    train_df.to_csv(os.path.join(save_path, \"train.csv\"), index=False)\n",
    "    valid_df.to_csv(os.path.join(save_path, \"valid.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(save_path, \"test.csv\"), index=False)\n",
    "    print(\"Train, Validation and Test datasets created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-kingdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagless set size: 0\n",
      "Tagless dataset created!\n",
      "Train set size: 2276\n",
      "Valid set size: 285\n",
      "Test set size: 285\n",
      "Train, Validation and Test datasets created!\n"
     ]
    }
   ],
   "source": [
    "splits = [0.8, 0.1]\n",
    "label = \"tagline\"\n",
    "seed = 42\n",
    "create_splits(df=df,\n",
    "              label=label,\n",
    "              splits=splits,\n",
    "              seed=seed,\n",
    "              keep_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525fb1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAI-Project",
   "language": "python",
   "name": "dsai-project"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
